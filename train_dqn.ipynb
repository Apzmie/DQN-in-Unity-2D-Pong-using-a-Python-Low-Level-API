{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "I trained on Google Colab to avoid overheating."
      ],
      "metadata": {
        "id": "eXItf0FWpyZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update -y\n",
        "!sudo apt-get install python3.10 python3.10-distutils -y\n",
        "\n",
        "!curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py\n",
        "!python3.10 get-pip.py\n",
        "\n",
        "!python3.10 --version\n",
        "!python3.10 -m pip --version\n",
        "\n",
        "!python3.10 -m pip install torch --index-url https://download.pytorch.org/whl/cu118\n",
        "!python3.10 -m pip install mlagents-envs matplotlib"
      ],
      "metadata": {
        "id": "DaBCXIBsewF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip a.zip"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Hnw_1xlfyxhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile train1.py\n",
        "from mlagents_envs.environment import UnityEnvironment\n",
        "from mlagents_envs.side_channel.engine_configuration_channel import EngineConfigurationChannel\n",
        "from mlagents_envs.base_env import ActionTuple\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "os.environ.pop(\"MPLBACKEND\", None)\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class ReplayMemory:\n",
        "    def __init__(self, capacity=100000, batch_size=32, w=0.5):\n",
        "        self.memory = deque(maxlen=capacity)\n",
        "        self.priorities = deque(maxlen=capacity)\n",
        "        self.batch_size = batch_size\n",
        "        self.w = w\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "        max_p = max(self.priorities, default=1.0)\n",
        "        self.priorities.append(max_p)\n",
        "\n",
        "    def sample(self):\n",
        "        weights = np.array(self.priorities)\n",
        "        probs = weights / weights.sum()\n",
        "        self.indices = np.random.choice(len(self.priorities), size=self.batch_size, replace=False, p=probs)\n",
        "        batch = [self.memory[i] for i in self.indices]\n",
        "        return batch\n",
        "\n",
        "    def update_priorities(self, delta_for_priorities):\n",
        "        for i, delta in zip(self.indices, delta_for_priorities):\n",
        "            self.priorities[i] = (abs(delta) + 1e-6) ** self.w\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, action_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_dim, action_dim, lr=1e-3, epsilon=1.0, min_epsilon=0.1, epsilon_decay_steps=200000.0, gamma=0.99):\n",
        "        self.q_net = QNetwork(state_dim, action_dim)\n",
        "        self.target_net = QNetwork(state_dim, action_dim)\n",
        "        self.target_net.load_state_dict(self.q_net.state_dict())\n",
        "        self.target_net.eval()\n",
        "        self.optimizer = torch.optim.Adam(self.q_net.parameters(), lr=lr)\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_start = epsilon\n",
        "        self.min_epsilon = min_epsilon\n",
        "        self.epsilon_decay_steps = epsilon_decay_steps\n",
        "        self.gamma = gamma\n",
        "        self.epsilon_decay = (self.epsilon_start - self.min_epsilon) / self.epsilon_decay_steps\n",
        "\n",
        "    def select_action(self, state, action_space, train=True):\n",
        "        if train and random.random() < self.epsilon:\n",
        "            action = random.choice(action_space)\n",
        "        else:\n",
        "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "            with torch.no_grad():\n",
        "                q_values = self.q_net(state_tensor)\n",
        "            action = int(torch.argmax(q_values, dim=-1).item())\n",
        "        return action\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        if self.epsilon > self.min_epsilon:\n",
        "            self.epsilon -= self.epsilon_decay\n",
        "            self.epsilon = max(self.epsilon, self.min_epsilon)\n",
        "\n",
        "    def train_step(self, batch, n_step):\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "        states = np.array(states, dtype=np.float32)\n",
        "        next_states = np.array(next_states, dtype=np.float32)\n",
        "        actions = np.array(actions, dtype=np.int64)\n",
        "        rewards = np.array(rewards, dtype=np.float32)\n",
        "        dones = np.array(dones, dtype=np.float32)\n",
        "\n",
        "        states = torch.from_numpy(states)\n",
        "        next_states = torch.from_numpy(next_states)\n",
        "        actions = torch.from_numpy(actions).unsqueeze(1)\n",
        "        rewards = torch.from_numpy(rewards).unsqueeze(1)\n",
        "        dones = torch.from_numpy(dones).unsqueeze(1)\n",
        "\n",
        "        q_values = self.q_net(states).gather(1, actions)\n",
        "        with torch.no_grad():\n",
        "            max_next_q_values = self.target_net(next_states).max(1, keepdim=True)[0]\n",
        "            target_q_values = rewards + (self.gamma ** n_step) * max_next_q_values * (1 - dones)\n",
        "\n",
        "        delta = target_q_values - q_values\n",
        "        delta_for_priorities = delta.detach().cpu().numpy().flatten()\n",
        "        delta = delta + (torch.clamp(delta, -1, 1) - delta).detach()\n",
        "        loss = (delta ** 2).mean()\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return delta_for_priorities\n",
        "\n",
        "    def update_target(self):\n",
        "        self.target_net.load_state_dict(self.q_net.state_dict())\n",
        "\n",
        "\n",
        "def collect_transitions(memory, env, behavior_name, action_space, replay_start_size=2000):\n",
        "    while len(memory) < replay_start_size:\n",
        "        env.reset()\n",
        "        decision_steps, _ = env.get_steps(behavior_name)\n",
        "        done = False\n",
        "        while not done:\n",
        "            state = decision_steps.obs[0][0]\n",
        "            action = random.choice(action_space)\n",
        "            action_tuple = ActionTuple(discrete=np.array([[action]], dtype=np.int32))\n",
        "            env.set_actions(behavior_name, action_tuple)\n",
        "            env.step()\n",
        "            decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
        "            if len(terminal_steps) > 0:\n",
        "                next_state = terminal_steps.obs[0][0]\n",
        "                reward = terminal_steps.reward[0]\n",
        "                done = True\n",
        "            else:\n",
        "                next_state = decision_steps.obs[0][0]\n",
        "                reward = decision_steps.reward[0]\n",
        "                done = False\n",
        "            memory.push(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            if len(memory) >= replay_start_size:\n",
        "                break\n",
        "    print(\"-\"*100)\n",
        "    print(f\"[Info] {len(memory)} transitions collected\")\n",
        "    print(\"-\"*100 + \"\\n\")\n",
        "\n",
        "\n",
        "def collect_eval_states(env, behavior_name, action_space, num_eval_states=500):\n",
        "    eval_states = []\n",
        "    while len(eval_states) < num_eval_states:\n",
        "        env.reset()\n",
        "        decision_steps, _ = env.get_steps(behavior_name)\n",
        "        done = False\n",
        "        while not done:\n",
        "            state = decision_steps.obs[0][0]\n",
        "            eval_states.append(state)\n",
        "            action = random.choice(action_space)\n",
        "            action_tuple = ActionTuple(discrete=np.array([[action]], dtype=np.int32))\n",
        "            env.set_actions(behavior_name, action_tuple)\n",
        "            env.step()\n",
        "            decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
        "            if len(terminal_steps) > 0:\n",
        "                next_state = terminal_steps.obs[0][0]\n",
        "                done = True\n",
        "            else:\n",
        "                next_state = decision_steps.obs[0][0]\n",
        "                done = False\n",
        "            state = next_state\n",
        "            if len(eval_states) >= num_eval_states:\n",
        "                break\n",
        "    print(\"-\"*100)\n",
        "    print(f\"[Info] {len(eval_states)} evaluation states collected\")\n",
        "    print(\"-\"*100 + \"\\n\")\n",
        "    return eval_states\n",
        "\n",
        "\n",
        "def compute_avg_max_q(agent, eval_states):\n",
        "    total_max_q = 0\n",
        "    with torch.no_grad():\n",
        "        for state in eval_states:\n",
        "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "            q_values = agent.q_net(state_tensor)\n",
        "            max_q = torch.max(q_values).item()\n",
        "            total_max_q += max_q\n",
        "    avg_max_q = total_max_q / len(eval_states)\n",
        "    return avg_max_q\n",
        "\n",
        "\n",
        "def plot_avg_max_q(episode_plot, avg_reward_plot, avg_max_q_plot, episode_count):\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15,5))\n",
        "    axes[0].plot(episode_plot, avg_reward_plot)\n",
        "    axes[0].set_xlabel('Episodes')\n",
        "    axes[0].set_ylabel('Average Reward')\n",
        "    axes[0].set_title('Average Reward per 100 Episode')\n",
        "    axes[0].grid(True)\n",
        "    axes[1].plot(episode_plot, avg_max_q_plot)\n",
        "    axes[1].set_xlabel('Episodes')\n",
        "    axes[1].set_ylabel('Average Max Q')\n",
        "    axes[1].set_title('Average Max Q per 100 Episode')\n",
        "    axes[1].grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"plot.png\")\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    channel = EngineConfigurationChannel()\n",
        "    channel.set_configuration_parameters(time_scale=20.0)\n",
        "    env = UnityEnvironment(file_name=\"a/PongBuild.x86_64\", side_channels=[channel], no_graphics=True)\n",
        "    env.reset()\n",
        "    available_behaviors = list(env.behavior_specs.keys())\n",
        "    behavior_name = available_behaviors[0]\n",
        "    spec = env.behavior_specs[behavior_name]\n",
        "\n",
        "    state_dim = spec.observation_specs[0].shape[0]\n",
        "    action_dim = spec.action_spec.discrete_branches[0]\n",
        "    agent = DQNAgent(state_dim, action_dim)\n",
        "    action_space = list(range(action_dim))\n",
        "    memory = ReplayMemory()\n",
        "    collect_transitions(memory, env, behavior_name, action_space)\n",
        "    eval_states = collect_eval_states(env, behavior_name, action_space)\n",
        "\n",
        "    episode_count = 0\n",
        "    total_steps = 0\n",
        "    best_test_reward = -float('inf')\n",
        "\n",
        "    state_skip = 4\n",
        "    target_update_step, next_target_update_step = 5000, 5000\n",
        "    test_reward_limit, test_reward_limit_increment = 200, 200\n",
        "    episode_count_interval = 100\n",
        "    recent_rewards = deque(maxlen=episode_count_interval)\n",
        "\n",
        "    episode_plot = []\n",
        "    avg_reward_plot = []\n",
        "    avg_max_q_plot = []\n",
        "\n",
        "    n_step = 3\n",
        "    n_step_buffer = deque(maxlen=n_step)\n",
        "\n",
        "    while True:\n",
        "        env.reset()\n",
        "        decision_steps, _ = env.get_steps(behavior_name)\n",
        "        done = False\n",
        "        episode_reward = 0\n",
        "        episode_count += 1\n",
        "\n",
        "        while not done:\n",
        "            state = decision_steps.obs[0][0]\n",
        "            action = agent.select_action(state, action_space)\n",
        "\n",
        "            skip_reward = 0\n",
        "            for _ in range(state_skip):\n",
        "                action_tuple = ActionTuple(discrete=np.array([[action]], dtype=np.int32))\n",
        "                env.set_actions(behavior_name, action_tuple)\n",
        "                env.step()\n",
        "                decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
        "                if len(terminal_steps) > 0:\n",
        "                    next_state = terminal_steps.obs[0][0]\n",
        "                    reward = terminal_steps.reward[0]\n",
        "                    done = True\n",
        "                else:\n",
        "                    next_state = decision_steps.obs[0][0]\n",
        "                    reward = decision_steps.reward[0]\n",
        "                    done = False\n",
        "                skip_reward += reward\n",
        "                total_steps += 1\n",
        "                agent.decay_epsilon()\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "            n_step_buffer.append((state, action, skip_reward, next_state, done))\n",
        "            if len(n_step_buffer) == n_step:\n",
        "                R = 0\n",
        "                for idx, (_, _, r, _, _) in enumerate(n_step_buffer):\n",
        "                    R += (agent.gamma ** idx) * r\n",
        "                s, a, _, _, _ = n_step_buffer[0]\n",
        "                _, _, _, last_next_state, last_done = n_step_buffer[-1]\n",
        "                memory.push(s, a, R, last_next_state, last_done)\n",
        "            if done:\n",
        "                while len(n_step_buffer) > 0:\n",
        "                    R = 0\n",
        "                    for idx, (_, _, r, _, _) in enumerate(n_step_buffer):\n",
        "                        R += (agent.gamma ** idx) * r\n",
        "                    s, a, _, n, d = n_step_buffer.popleft()\n",
        "                    if len(n_step_buffer) > 0:\n",
        "                        _, _, _, last_next_state, last_done = n_step_buffer[-1]\n",
        "                    else:\n",
        "                        last_next_state, last_done = n, d\n",
        "                    memory.push(s, a, R, last_next_state, last_done)\n",
        "\n",
        "            episode_reward += skip_reward\n",
        "            state = next_state\n",
        "\n",
        "            if len(memory) >= memory.batch_size:\n",
        "                batch = memory.sample()\n",
        "                delta_for_priorities = agent.train_step(batch, n_step)\n",
        "                memory.update_priorities(delta_for_priorities)\n",
        "\n",
        "            if total_steps >= next_target_update_step:\n",
        "                agent.update_target()\n",
        "                next_target_update_step += target_update_step\n",
        "\n",
        "                test_reward = 0\n",
        "                env.reset()\n",
        "                decision_steps, _ = env.get_steps(behavior_name)\n",
        "                test_done = False\n",
        "                while not test_done:\n",
        "                    state = decision_steps.obs[0][0]\n",
        "                    action = agent.select_action(state, action_space, train=False)\n",
        "                    action_tuple = ActionTuple(discrete=np.array([[action]], dtype=np.int32))\n",
        "                    env.set_actions(behavior_name, action_tuple)\n",
        "                    env.step()\n",
        "                    decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
        "                    if len(terminal_steps) > 0:\n",
        "                        next_state = terminal_steps.obs[0][0]\n",
        "                        t_reward = terminal_steps.reward[0]\n",
        "                        test_done = True\n",
        "                    else:\n",
        "                        next_state = decision_steps.obs[0][0]\n",
        "                        t_reward = decision_steps.reward[0]\n",
        "                        test_done = False\n",
        "                    test_reward += t_reward\n",
        "                    state = next_state\n",
        "\n",
        "                    if test_reward >= test_reward_limit:\n",
        "                        test_done = True\n",
        "                        test_reward_limit += test_reward_limit_increment\n",
        "\n",
        "                print(\"=\"*100)\n",
        "                print(f\"[Test] Episode {episode_count}: Test reward = {test_reward}\")\n",
        "                if test_reward > best_test_reward:\n",
        "                    best_test_reward = test_reward\n",
        "                    print(f\"[Test] Episode {episode_count}: New best test reward = {best_test_reward}\")\n",
        "                    torch.save(agent.q_net.state_dict(), \"best_model.pth\")\n",
        "                    print(f\"[Test] Episode {episode_count}: Model saved as 'best_model.pth'\")\n",
        "                print(\"=\"*100 + \"\\n\")\n",
        "\n",
        "        if episode_count % episode_count_interval == 0:\n",
        "            print(\"-\"*100)\n",
        "            print(f\"[Info] Episode {episode_count} in progress, Step {total_steps}, Epsilon {agent.epsilon}\")\n",
        "            print(\"-\"*100 + \"\\n\")\n",
        "            episode_plot.append(episode_count)\n",
        "            recent_rewards.append(episode_reward)\n",
        "            avg_reward = sum(recent_rewards) / len(recent_rewards)\n",
        "            avg_reward_plot.append(avg_reward)\n",
        "            avg_max_q = compute_avg_max_q(agent, eval_states)\n",
        "            avg_max_q_plot.append(avg_max_q)\n",
        "            plot_avg_max_q(episode_plot, avg_reward_plot, avg_max_q_plot, episode_count)"
      ],
      "metadata": {
        "id": "C3ceS_QMa3sc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3.10 train1.py"
      ],
      "metadata": {
        "id": "ocwaNqzP7wjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Disconnect the runtime and reconnect."
      ],
      "metadata": {
        "id": "-2W3bV8muth7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade onnx"
      ],
      "metadata": {
        "id": "h4ARjSSykeCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import onnx\n",
        "\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, action_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "state_dim = 5\n",
        "action_dim = 3\n",
        "\n",
        "model = QNetwork(state_dim, action_dim)\n",
        "state_dict = torch.load(\"best_model.pth\", weights_only=True)\n",
        "model.load_state_dict(state_dict)\n",
        "model.eval()\n",
        "\n",
        "dummy_input = torch.randn(1, state_dim)\n",
        "\n",
        "torch.onnx.export(\n",
        "    model,\n",
        "    dummy_input,\n",
        "    \"best_model.onnx\",\n",
        "    export_params=True,\n",
        "    opset_version=15,\n",
        "    do_constant_folding=True,\n",
        "    input_names=['X'],\n",
        "    output_names=['Y'],\n",
        "    dynamo=False\n",
        ")"
      ],
      "metadata": {
        "id": "5bj1mnZ-Y-EM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}